# Introduzione
L'indicizzazione del web introduce una serie di sfide aggiuntive dovute al fatto che i documenti indicizzati sono "non-standard", portano con se più informazioni, come i link.

Disciplina della **link analysis**
## Storia del web
Primi search engine **keyword-based** - Altavista, Excite, Infoseek, Inktomi, Lycos - nascono nel 95-97.

goto.com -> overture.com -> Yahoo

Prime idee: inverted index + rank proporzionale all'**acquisto** di termini. Borsa dei termini. Valore esploso di casinò.

1998: link-based ranking di Brin e Page. Introducono i risultati pubblicizzati _ads_, slegati dall'indice vero e proprio, costituendo l'indice ADS, da cui vengono prelevati risultati pubblicitari rilevanti per la query e mostrati in alto durante la ricerca. 

>Search-engine **verticale**: per utenti di tutti i tipi. Non hai in mente un tipo di utente specifico.
>**Spider**: servizio dell'indexer che esplora la "rete" di pagine web

Necessità di navigazione:
- **informative**: conoscere qualcosa
- **navigative**: arrivare a una pagina
- **transazionali**: acquistare, scaricare file, accedere a servizi, ecc. transitando per il motore di ricerca
- **aree grigie**: non ben identificabili, come locali nell'area corrente e simili

## Caratteristiche del web
- non esiste un coordinamento generale
- il dato passa da non strutturato a strutturato
- in continua evoluzione: dinamico
- non moderato

La teoria dei grafi ha cercato di caratterizzare la topologia del web: papillon -> **bow-tie structure**
- componenti disconnesse: 17M vertici
- dendriti: 44M vertici
- core fortemente connesso: 56M vertici
- due capi "mediamente connessi" che passano per il core e sono interconnessi da questo, da sinistra destra, chiamati _in-components_ e _out-components_

Circa il 40% delle pagine è sintatticamente duplicato (**mirroring**)

# Primi problemi
Cercando parole come "toyota", nelle prime posizioni comparivano le pagine con molte occorrenze di questa, a causa della rappresentazione vector-space.

Alcune pagine aggiravano addirittura il ranking inserendo centinaia di parole sullo sfondo, in formato non human-readable.

Page e Brin si accorgono che manca il concetto di **autorevolezza**. Non è la pagina stessa che può dare un'idea della sua autorevolezza.

Idea: l'importanza di una pagina è proporzionale dal numero di link entranti (**inlink** portati da **ancore**). Il ranking diventa così basato sulla **topologia** del web, non sul suo contenuto. La link analysis nasce. Viene introdotto il concetto di **trustness**. Un link da p a q denota che p si fida di q.

L'informazione di trustness si estrae mediante graph mining.
**LAR** - **Link Analysis Ranking** - algorithm: si parte da una collezione di documenti, si estraggono gli hyperlink di ogni documenti, si genera il grafo. A questo punto su ogni vertice si può applicare l'algoritmo LAR. Il risultato è il grado di autorevolezza di ogni pagina.

# Algoritmi LAR
In base all'inseme di input dell'algoritmo, si classificano in:
- ==PageRank== - Google '98 - **query-independent** (calcolabile **offline**, indicizzando tutto Internet)
- ==HITS== - Kleinberg - **query-dependent**. Parte da un piccolo sottoinsieme di pagine rilevanti per la query.

https://www.cs.cornell.edu/home/kleinber/networks-book/

## Page Rank
>Si basa sul **random surfer model**: modella un utente che naviga il web seguendo le ancore posizionate nella pagina che sta visitando o aprendo pagine web ex-novo, per esempio mediante bookmarks. Il modello tiene conto del fatto che l'utente, dopo K link seguiti, smetterà di navigare i vicini, aprendo una pagina totalmente nuova. Inoltre, il modello ritiene equamente probabile il click su uno dei link contenuti in una pagina, ignorandone quindi il contenuto.

Idea: probabilità di raggiungere una pagina del web partendo da un'altra pagina del web.
Autorità (pagina autorevole): pagina che ha dimostrato di fornire informazioni significative e affidabili su un argomento.

Si considera una **probabilità uniforme** di cliccare ogni outlink della pagina.

$$R(p) = c \sum_{q: q\rightarrow p}\frac{R(q)}{N_q}$$
La probabilità di raggiungere p è la sommatoria delle probabilità di raggiungere p a partire da ogni pagina q con un link uscente verso p fratto il numero totale di outlink. Anche la pagina sorgente deve essere autorevole. $c$ è un valore di normalizzazione

## Pseudocodice
### Versione iterativa - sistema dinamico
Versione iterativa:
1. Dato S insieme totale delle pagine
2. Inizializzo $R_0(p) = \frac{1}{|S|} \forall p \in S$
3. Inizializzo $i = 0$
4. Itero
	1. incremento i
	2. Per ogni pagina $p \in S$: $R(p) = c \sum_{q: q\rightarrow p}\frac{R_{i-1}(q)}{N_q}$
	3. Normalizzazione -> ricalcolo $c$ per ogni pagina $p$: $c = \frac{1}{\sum_p R_i(p)}$
	4. Se $|R_i(p) - R_{i-1}| \le \epsilon$ mi fermo -> tolleranza raggiunta

L'approccio iterativo prevede la costruzine di una matrice di adiacenza $A$ (variante per grafi diretti pesati) che rappresenti i link tra le pagine. Sulla diagonale principale tutti zero.
Si inizializza un vettore della trustness $v$ uguale a $\frac{1}{|S|}$. Si calcola il prodotto $v = Av$ fino al raggiungimento di un punto stabile (lo scarto tra due iterate successive è "piccolo").

### Versione algebrica - sistema di equazioni
Si imposta un sistema di equazioni lineari nella forma: $$\begin{cases} x_1 = x_3 + \frac{1}{x_2} \\ ... \\ x_4 = \frac{1}{3} x_1 + \frac{1}{4} x_2 \end{cases}$$
Dove $x_i$ è la trustness della i-esima pagina del dataset. I coefficienti sono dati dalla matrice A vista sopra. Si cerca la soluzione di $Ax = x$.
## Problemi possibili
Problemi possibili:
- **cicli** o pozzi di trustness
- componenti disconnesse: portano ad una soluzione del sistema ambigua; per risolvere viene introdotto il _damping factor_ $\alpha$ visto sotto
### Componenti disconnesse e Damping factor
>Il damping factor modella il "teletrasporto" del random surfer ad un'altra pagina completamente scorrelata dal flusso dei link. Risolve inoltre il problema delle componenti disconnesse: la loro unica possibilità è infatti che vengano raggiunge con un cambio di pagina dal random surfer.

La matrice di adiacenza, chiamata Google matrix, diventa quindi: $$M = (1-\alpha) A + \alpha B$$ con $B = \frac{1}{N} I$

$B$ viene scelta in questo modo perché la probabilità di salto è uniforme $\frac{\alpha}{N}$
Durante le iterazioni M del passo i-1 sostituisce A nel passo i.
### Trustness sink e Rank source
Il problema di cui sopra, visto da un punto di vista differente.
Dato un insieme finito di pagine, opportunamente accordate, creando un ciclo tra di esse (constatato che esista almeno un arco entrante), è possibile _assorbire_ tutta l'autorevolezza del web. Problema del **pozzo infinito**.

Risoluzione: si introduce una _rank source E_ che rifornisce il ranking di ogni pagina $p$ di una quantità fissa:
$$R_i(p) = (1-\alpha) \sum_{q: q\rightarrow p}\frac{R_{i-1}(q)}{N_q}+\alpha E(p)$$
Dato $0 < \alpha < 1$ tipicamente fissato a $0.15$

In questo caso l'inizializzazione avviene con $\frac{\alpha}{|S|}$

### Personalizzazione
Sulla base della cronologia di ricerca, un valore di $E(p)$ non uniforme permette di personalizzare la rilevanza dei contenuti rankati. Viene associato un valore più alto alle pagine maggiormente frequentate/potenzialmente preferite.

Per esempio, si potrebbe impostare $E(p) = 0$ per tutte le pagine, a parte per una pagina $E(p) = \alpha$ da cui parte lo spreading del valore.
## Interpretazione probabilistica
L'algoritmo di PR può essere modellato dal punto di vista probabilistico come un algoritmo che assegna ad ogni pagina un rank proporzionale alla probabilità che un random surfer, aprendo un browser, visiti una certa pagina. La posizione della pagina è legata alla sua popolarità (numero di inlink e popolarità dei referees).

## Costi
Velocità di convergenza: empiricamente logaritmica sul numero di link. Esistono tecniche di calcolo distribuito per PR.

Il numero di iterazioni richiesto è $O(logE)$ - E significa Edges

Perché ci interessa ricalcolarlo? perché il web è dinamico. Ogni settimana il 25% di nuovi link è creato. Vale la legge dell'80-20: il 20% delle pagine subisce l'80% delle modifiche.

## HITS - Hubs and Authorities
Algoritmo di ranking LAR ì, query-dependent, inventato da Kleinberg (Cornell University) nel 1998.

Divide il web in:
- ==Authority==: forniscono informazioni riguardo ad un argomento
- ==Hub==: pagine che forniscono poche informazioni, ma rimandano ad altre pagine

La classificazione non è netta, una pagina può essere sia hub che authority.
Idea: hub di qualità puntano a autorità di qualità

Ogni pagina $v$ ha due valori:
- $h(v)$ hubness: importanza di una pagina - alta se punta a molti siti autorevoli
- $a(v)$ authority: alta se molto citata (citazione pesata in base all'autorevolezza del nodo sorgente)

La definizione di $a$ fa uso degli $h$ delle pagine referenti. La definizione di $h$ fa uso delle $a$ delle pagine riferite.

### Pseudocodice
1. Data la query Q, trovare tutte le pagine rilevanti, che costituiranno il **root set**
2. Trovare tutti i _forward neighbours_, ovvero tutte le  pagine puntate dalle pagine del root set
3. Trovare tutti i _backward neighbours_, ovvero tutte le pagine che puntano a pagine del root set
4. Calcolare il sottografo di queste pagine, chiamato **Base set**
5. Inizializzare hub e auth a 1
6. Per ogni pagina calcolare $h(v) = \sum_{u :  v \rightarrow u} a(u)$ e $a(v) = \sum_{u :  u \rightarrow v} h(u)$ come valore normalizzato
7. Itera fino alla convergenza

### Spamming
È un algoritmo facile da ingannare: creato un hub ad-hoc, con riferimenti a molti siti autorevoli, e aggiunti riferimenti "fuori contesto"/bassa autorevolezza, anch'essi diventeranno pagine accreditate.

### Riepilogo
Caratteristiche di HITS:
- topic-drift del base set
- query dependence
- non troppo dispendioso su un insieme piccolo di pagine

## Spamming and SEO
Search Engine Optimization - alternativa gratuita al paid placement.

Tecniche:
- keyword stuffing - funzionava nei primi motori di ricerca content-based, in cui TF aveva un peso alto
- cloaking - servire un contenuto diverso agli spider
- doorway page - pagina specializzata su alcune keyword che reindirizzano a quella reale
- link spamming - domain flooding, mutual admiration societies, hidden links, ...
- robots - fake queries stream, ...

