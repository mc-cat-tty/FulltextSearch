# Thesaurus
>Un **thesaurus** è una raccolta di parole appartenenti ad uno stesso dominio di conoscenza. Per ogni parola viene memorizzata una lista di parole correlate da relazioni di sinonimia, iponimia e iperonimia.

*Iponimo*: auto è un iponimo di mezzo di trasporto
*Iperonimo*: mezzo di trasporto è iperonimo di auto

#Ricorda con ipo (sotto) e iper (sopra) - ipovedente, ecc.

> **Polisemia** = relazione tra parole identiche (stessa grafia) ma con semantica diversa

Esempi di thesaurus:
- Roget (primo dizionario, 1852)
- Wordnet
- INSPEC
- MESH

#Attenzione il plurale di *thesaurus* è *thesauri*
#Vedi Pubmed e MeSH (Ontologia terminologica): https://www.ncbi.nlm.nih.gov/mesh/

Perché usare thesaurus? avere un vocabolario controllato, affidabile, con basso rumore

Non necessari per ricerche generiche, sono utili soprattutto per i campi ristretti/settoriali

## TIT - Thesaurus Index Term
#Attenzione non è detto che un concetto sia espresso da una sola parola

Nei thesaurus il dato strutturato da essi memorizzato è: **concetto** (unità semantica di base, costituito da una parola, più parole o frasi) - **definizione** (spiegazione)

TIT relationship -> relazioni di generalizzazione (iperonimia) -  specializzazione (iponimia) e, sullo stesso livello sinonimia o appartenenza allo stesso oggetto/ambito/ecc.

Esempio di *mouse*, parola polisemica: http://wordnetweb.princeton.edu/perl/webwn?s=mouse&sub=Search+WordNet&o2=&o0=1&o8=1&o1=1&o7=&o5=&o9=&o6=&o3=&o4=&h=

**Grafo bipartito** (grafo i cui vertici possono essere suddivisi in due insiemi senza che vertici dello stesso insieme siano connessi tra loro) che collega concetti a *synset*

## Relazioni semantiche tra le parole
I synset catturano relazioni di sinonimia; i concetti connessi a più synset sono polisemici.

Altre relazioni semantiche sono:
- **meronimia**: una entità è parte di un'altra
- **olonimia**: un'entità ne contiene un'altra

## Creazione di un thesaurus
Manualmente si ha lo svantaggio di:
- interpretazione soggettiva della semantica
- time consuming

Selezione automatica:
- dipende dalla lingua: 1 thesaurus per lingua
- copertura
- tecniche di **Word Sense Disambiguation - WSD**

# Similarità delle parole - Word Similarity
>È un rilassamento del concetto di sinonimia secondo una metrica ordinabile e comparabile.
>La sinonimia è un concetto binario.

Utile per:
- question answering
- machine translation
- modelli generativi
- ...

Es:
- *car* e *bicycle* simili
- *car* e *gasoline* no

Storicamente due metodi:
- **path-based**: risalendo la gerarchia di iperonimia delle due parole, finchè non si incontrano sotto un "cappello" comune (il più basso antenato comune)
- **information-content**: quanto i due significati vengono usati in situazioni simili in corpus di riferimento. Quanto appaiono insieme, nello stesso contesto. Vedi w2v.

## Path-based
### Path distance similarity
Basata sullo shortest-path che connette le semantiche delle parole attraverso una gerarchia *is-a* (gerarchia di iponimima-iperonimia)

$$
sim_{path\_distance} (c_1, c_2) = \frac{1}{shortest\_path(c_1, c_2) +1}
$$
#Nota che il range è  da 0 a 1
#Problema: *shortest_path* può assumere solo valori discreti, in N, quindi il valore risultate è sempre più ammassato per i valori alti di *shortest_path*

La $shortest_path(c_1, c_2) = 0$

### Wu-Palmer distance
*Lowest Common Subsumer* (il più specifico antenato comune dei due concetti)
$$
sim_{wu\_palmer}(c_1, c_2) = \frac{2*depth(LCS(c_1, c_2))}{depth(c_1) + depth(c_2)} 
$$

L'idea della distanza di Wu-Palmer è quella di considerare anche la specificità di termini, ovvero quanto "albero" i termini hanno già sopra la testa. Con specificità elevate inizia a pesare meno la distanza relativa tra i due concetti. Infatti, ponenedo:
- $k$ uguale alla profondità del $LCS(c_1, c_2)$ -> i nodi che si trovano sopra all'LCS
- $i$ uguale alla distanza LCS-c1
- $j$ uguale alla distanza LCS-c2

La formula può essere riscritta come:
$$
sum_{wu\_palmer} = \frac{2k}{i+j+2k}
$$

Con k alti la similarità tende a 1. Con $i=j$ la similarità tende a 1.

## Information-content similarities
$$
IC(c) = -log(P(c))
$$
**Information content** o **Shannon information**: è una misura di probabilità che quantifica il livello di sorpresa di un possibile risultato. Alcune osservazioni:
- un evento certo (P=1) non trasporta informazione, quindi il suo livello di sorpresa è 0
- Più un evento è raro più è sorprendente
- Se due eventi indipendenti sono misurati separatamente il livello di sorpresa è la somma dei due livelli di sorpresa dei singoli eventi

Nel caso sopra: livello di sorpresa di trovare il concetto *c* nel corpus di documenti. O meglio, P(c) è la probabilità che, presa una parola a caso nel corpus, essa sia un'istanza del concetto c.

IC è alta quanto il termine è usato raramente, è bassa se il concetto è di uso comune

### Resnik similarity
È basato sull'IC del LCS - Lower Common Subsumer

$$
sim_{resnik}(c_1, c_2) = -logP(LCS(c_1, c_2))
$$

#Vedi paper *Evolution of semantic similarity  - A Survey*

Più i concetti sono distanti tra loro, più il LCS sarà alto, quindi la probabilità di trovarli insieme sarà bassa (ricorda curva -log)

# WSD: Word Sense Disambiguation
>La WSD si prepone di assegnare ad ogni parola di un testo il suo corretto significato/la sua corretta definizione.

Due fasi:
1. determinare tutti i possibili significati di una parola presente in un testo
2. ranking dei sensi -> l'algoritmo di WSD fornisce una lista di significati possibili, ognuno con la propria confidenza
3. assegnare ogni occorrenza ad un signficato appropriato

La confidenza viaggia nell'intervallo \[0, 1\], dove a valori alti corrisponde una probabilità alta di correttezza.

Serve scegliere un thesaurus e sulla base di questa scelta analizzare il contesto. Due approcci alternativi:
- **bag of words**: il contesto è una finestra di parole che circondano la parola da disambiguare (sliding window)
- **relational information approach**: il contesto, si porta dietro anche la distanza delle parole dal termine da disambiguare
Il primo approccio si usa solitamente per testi brevi, dove ci si aspetta che tutto il testo parli dello stesso argomento. Per testi lunghi o articolati si usa il secondo approccio.

Il secondo approccio smorza il peso della probabilità attraverso la distanza del termine dalla parola del contesto -> ad esempio mettendo la distanza al denominatore (**inverso della distanza** che può essere mitigata con il logaritmo)

## Naive approach
Fissato un nome N da disambiguare. Per ogni significato $S_N$ di N, per ogni significato di ogni parola del contesto, calcolo la confidenza di significato di $S_N$ come il valore di similarità più alto tra i due significati -> ovvero prendo la coppia di significati con LCS più basso.
Prodotto cartesiano di tutti i significati di tutte le parole della bag of words.

Più rigorosamente:
Data una lista di termini $t_1, t_2, ..., t_n$ che costituisce il corpo del documento.
- $\forall t \in context$
	- $\forall s \in t, conf{tmp}=0$
		- $\forall t' \in text : t' \neq t$
			- $conf_{tmp} += max\{sim(s, s') : s' \in t'\}$
	- $conf_s = max\{conf_{tmp}\}, s = argmax\{conf_{tmp}\}$

## Tecniche allo stato dell'arte
Si usano gli LLM - Large Language Models - che sono fondamentalmente transformer per generare un vettore di parole "di contesto" per i diversi significati.
# Query expansion
>Processo di fornire termini supplementari al search engine arricchendo la frase di ricerca con sinonimi dei termini utilizzati per la query


#Vedi YAKE, RAKE