# Thesaurus
*Iponimo*: auto è un iponimo di mezzo di trasporto
*Iperonimo*: mezzo di trasporto è iperonimo di auto

#Ricorda con ipo (sotto) e iper (sopra) - ipovedente, ecc.

> **Polisemia** = relazione tra parole identiche ma con semantica diversa

Esempi di thesaurus:
- Roget (primo dizionario, 1852)
- Wordnet
- INSPEC
- MESH

#Attenzione il plurale di *thesaurus* è *thesauri*
#Vedi Pubmed e MeSH (Ontologia terminologica): https://www.ncbi.nlm.nih.gov/mesh/

Perché usare thesaurus? avere un vocabolario controllato, affidabile, con basso rumore

Non necessari per ricerche generiche, sono utili soprattutto per i campi ristretti/settoriali

## TIT - Thesaurus Index Term
#Attenzione non è detto che un concetto sia espresso da una sola parola

Nei thesaurus il dato strutturato da essi memorizzato è: concetto (unità semantica di base, costituito da una parola, più parole o frasi) - definizione (spiegazione)

TIT relationship -> relazioni di generalizzazione (iperonimia) -  specializzazione (iponimia) e, sullo stesso livello, appartenenza allo stesso oggetto/ambito/ecc.

Esempio di *mouse*, parola polisemica: http://wordnetweb.princeton.edu/perl/webwn?s=mouse&sub=Search+WordNet&o2=&o0=1&o8=1&o1=1&o7=&o5=&o9=&o6=&o3=&o4=&h=

**Grafo bipartito** che collega parole, concetti a *synset*

## Relazioni semantiche tra le parole
- Hypernymy
- Meronymy -> una parte usata per descrivere il tutto: facce per persone (parte del nome)
- Is-value-of -> parte di (oggetti di cui è parte)
- synonymy -> stesso significato

## Creazione di un thesaurus
Manualmente si ha lo svantaggio di:
- interpretazione soggettiva della semantica
- time consuming

Selezione automatica:
- dipende dalla lingua: 1 thesaurus per lingua
- copertura
- tecniche di **Word Sense Disambiguation - WSD**

# Similarità delle parole
Diversa da sinonimia
*World similarities*

Utile per:
- question answering
- machine translation
- modelli generativi
- ...

Es:
- *car* e *bicycle* simili
- *car* e *gasoline* no

È un rilassamento del concetto di sinonimia. Metrica ordinabile e comparabile.

Storicamente due metodi:
- **path-based**: risalendo la gerarchia di iperonimia delle due parole, finchè non si incontrano sotto un "cappello" comune
- **information-content**: quanto i due significati vengono usati in situazioni simili in corpus di riferimento. Quanto appaiono insieme, nello stesso contesto.

## Path-based
### Path distance similarity
Basata sullo shortest-path che connette le semantiche delle parole attraverso una gerarchia *is-a*

$$
sim_{path\_distance} (c_1, c_2) = \frac{1}{shortest\_path(c_1, c_2) +1}
$$
#Nota che il range è  da 0 a 1
#Problema: *shortest_path* può assumere solo valori continui, in N, quindi il valore risultate è sempre più ammassato per i valori alti di *shortest_path*

### Palmer distance
*Lowest Common Subsumer* (il più specifico antenato comune dei due concetti)
$$
sim\_palmer(c_1, c_2) = \frac{2*depth(LCS(c_1, c_2))}{depth(c_1) + depth(c_2)} 
$$

## Information-content similarities
$$
IC(c) = -log(P(c))
$$
**Information content** o **Shannon information**: livello di sorpresa di trovare il concetto *c* nel corpus del documento
#Nota prova a plottarla

La sorpresa è molto alta (tende a +inf) per probabilità basse. Per la probabilità certa (P(c) = 1) ho sorpresa uguale a zero.

IC è bassa quanto il termine è usato raramente, è alta se il concetto è di uso comune

### Resnik similarity
È basato sull'IC del LCS - Lower Common Subsumer

$$
sim_{resnik}(c_1, c_2) = -logP(LCS(c_1, c_2))
$$

#Vedi paper *Evolution of semantic similarity  - A Survey*