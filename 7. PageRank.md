# Introduzione
L'indicizzazione del web introduce una serie di sfide aggiuntive dovute al fatto che i documenti indicizzati sono "non-standard", portano con se più informazioni, come i link.

Disciplina della **link analysis**

## Storia del web
Primi search engine keyword-based: Altavista, Excite, Infoseek, Inktomi, Lycos.
Nascono nel 95-97

goto.com -> overture.com -> Yahoo

Idea di base: inverted index + rank proporzionale all'acquisto di termini. Borsa dei termini. Valore esploso di casinò.

1998: link-based analysis di Brin e Page

>Search-engine **verticale**: per utenti di tutti i tipi. Non hai in mente un tipo di utente specifico.
>**Spider**: servizio dell'indexer che esplora la "rete" di pagine web

Necessità di navigazione:
- informative: conoscere qualcosa
- navigative: arrivare a una pagina
- transazionali: acquistare, scaricare file, ecc.
- aree grigie: non ben identificabili, come locali nell'area corrente e simili

## Caratteristiche del web
- non esiste un coordinamento generale
- il dato passa da non strutturato a strutturato
- in continua evoluzione
- non moderato

La teoria dei grafi ha cercato di caratterizzare la topologia del web: papillon
- componenti disconnesse
- core fortemente connesso
- due capi che passano per il core e sono interconnessi da questo

Circa il 40% delle pagine è sintatticamente duplicato (mirroring)

# Primi problemi
Cercando parole come "toyota", nelle prime posizioni comparivano le pagine con molte occorrenze di questa, a causa della rappresentazione vector-space.

Alcune pagine aggiravano addirittura il ranking inserendo centinaia di parole sullo sfondo, in formato non human-readable.

Page e Brin si accorgono che manca il concetto di **autorevolezza**. Non è la pagina stessa che può dare un'idea della sua autorevolezza.

Idea: l'importanza di una pagina è proporzionale dal numero di link entranti (**inlink** portati da **ancore**). Il ranking diventa così basato sulla topologia del web, non sul suo contenuto. La link analysis. Viene introdotto il concetto di **trustness**. Un link da p a q denota che p si fida di q.

L'informazione di trustness si estrae mediante graph mining.

LAR - Link Analysis Ranking - algorithm: si parte da una collezione di documenti, si estraggono gli hyperlink di ogni documenti, si genera il grafo. A questo punto su ogni vertice si può applicare l'algoritmo LAR.

# Algoritmi
- PageRank - Google '98 - query independent (calcolabile offline :)
- HITS - Kleinberg - query-dependent

https://www.cs.cornell.edu/home/kleinber/networks-book/

## Page Rank
>Si basa sul **random surfer model.

Idea: probabilità di raggiungere una pagina del web partendo da un'altra pagina del web.

Si considera una probabilità uniforme di cliccare ogni outlink della pagina.

$$R(p) = c \sum_{q: q\rightarrow p}\frac{R(q)}{N_q}$$
La probabilità di raggiungere p a partire da q è la probabilità di raggiungere q fratto il numero totale di outlink. Anche la pagina sorgente deve essere autorevole. $c$ è un valore di normalizzazione

Problemi possibili:
- cicli. Risolvibili con
	- approccio algebrico -> autovalori e autovettori
	- approccio algoritmico -> ricerca di punti stabili
- punto di partenza

### Risoluzione dei cicli
1. Dato S insieme totale delle pagine
2. Inizializzo $R_0(p) = \frac{1}{|S|} \forall p \in S$
3. Inizializzo $i = 0$
4. Itero
	1. incremento i
	2. Per ogni pagina $p \in S$: $R(p) = c \sum_{q: q\rightarrow p}\frac{R_{i-1}(q)}{N_q}$
	3. Normalizzazione -> ricalcolo $c$
	4. #Completa 

### Ulteriori problemi
Dato un insieme finito di pagine, opportunamente accordate, creando un ciclo tra di esse (constatato che esista almeno un arco entrante), è possibile _assorbire_ tutta l'autorevolezza del web. Problema del **pozzo infinito**.

Risoluzione: si introduce una _rank source E_ che rifornisce il ranking di ogni pagina $p$ di una quantità fissa:
$$R_i(p) = (1-\alpha) \sum_{q: q\rightarrow p}\frac{R_{i-1}(q)}{N_q}+\alpha E(p)$$
Dato $0 < \alpha < 1$
### Interpretazione probabilistica
L'algoritmo di PR può essere modellato dal punto di vista probabilistico come un random surfer.

### Algoritmo rivisitato
#Completa slide 30

1. S set di pagine
2. $\forall p \in S: \frac{E(p)}{|S|}$

### Personalizzazione
Sulla base della cronologia di ricerca, il valore $E(p)$ permette di personalizzare la rilevanza dei contenuti rankati. Viene associato un valore più alto alle pagine maggiormente frequentate/potenzialmente preferite.

Per esempio, si potrebbe impostare $E(p) = 0$ per tutte le pagine, a parte per una pagina $E(p) = \alpha$ da cui parte lo spreading del valore.

### Costi
Velocità di convergenza: logaritmica sul numero di link. Esistono tecniche di calcolo distribuito per PR.

Perché ci interessa ricalcolarlo? perché il web è dinamico. Ogni settimana il 25% di nuovi link è creato. Vale la legge dell'80-20: il 20% delle pagine subisce l'80% delle modifiche.

