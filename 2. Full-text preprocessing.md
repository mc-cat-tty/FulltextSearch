# Overview
>Il preprocessing di un documento lo trasforma in una lista di termini da indicizzare.

#x006

Comprende le seguenti fasi:
1. Analisi lessicale
2. Rimozione delle stopwords
3. Stemming o lemmatizzazione delle parole restanti
4. Scelta degli _index term_
5. Creazione di strutture aggiuntive sui termini, per esempio per permettere il match di sottostringhe o query di prossimità
# Analisi lessicale di un testo
>La **tokenizzazione/analisi lessicale** è la prima fase del preprocessing di un testo.

L'analisi semantica di un testo in linguaggio naturale è meno esatta di quella compiuta da un compilatore.

>Cosa è un token? è un gruppo di caratteri con significato collettivo.

Solitamente si scartano i numeri, il trattino è separatore (hyphenated words) a parte in casi eccezionali, punto rimosso a parte se appartiene a dominio o simili (indirizzi IP), `tolower` a parte se viene modificata la semantica del testo.

A differenza di un linguaggio di programmazione (testo libero), il full-text è testo libero.

>*Corpus di documenti*: Ad esempio documenti scientifici richiedono di tenere in conto dei numeri.

## Eliminazione delle stopwords
Parole che hanno un **basso valore discriminante** (TF-IDF bassa, perché presenti in molti documenti). Sono congiunzioni (però, ma, sebbene, seppur, ...), articoli, dimostrativi (these, those, that, this) e tutti i lessemi necessari a dare senso compiuto ad una frase.


Si possono rimuovere nel lexer o si può fare un filtraggio prima. Si basa su un dizionario di *stopwords*.

## Stemming & lemmatization
>Idea: trovare **varianti morfologiche** di una parola in fase di ricerca

Due modi di fare questo:
- **stemming** (da *stem*, ovvero stelo): portare ogni parola ad una forma in cui vengono rimossi tutti gli **affissi**, ovvero prefissi e suffissi della parola. Forma più grezza. Funziona principalmente in inglese, perché ci sono poche eccezioni/forme speciali.
- **lemmatization**: trasformare ogni parola nella sua forma base (appunto il **lemma**) mediante l'aiuto di un dizionario. Es: *saw* -> *see*
>**Affisso**: morfema legato ad una parola che, aggiunto alla radice, la rende morfologicamente più complessa.

#Attenzione si devono scegliere sia stemmer e lemmatizer coerenti tra fase di ricerca e fase di preprocessing della text repository

Vantaggi degli stemmer
- compressione della dimensione dell'indice (rapporto di compressione che può raggiungere il 50%)
- gli stemmer possono migliorare in positivo l'efficacia di ricerca, portando più parole a una forma comune

Svantaggi degli stemmer:
- probabilmente incorretto. Problemi di understemming e overstemming

# Index term selection
> Non tutte le parole sono equamente importanti nella contribuzione alla semantica di una frase/documento

Metodi:
- selezione manuale effettuata da *language engineers* con strumenti di supporto
- selezione automatica attraverso dizionari di nomi o simili <- la maggior parte della semantica è trasportata dai nomi, individuati da parser e tagger

# Parser e Tagger
> Un **parser sintattico** è un tool che assegna una struttura sintattica ad una frase: riconosce parti della frase (**part of speech**) etichettandole e costruisce un albero (**albero sintattico**) con le parti della frase o con le frasi del documento.

L'approccio più efficace al parsing è quello statistico mediante l'uso di **treebank**: corpus di testo annotati con POS, su cui è stato costruito un albero sintattico (da qui il nome tree). A volte sono chiamati anche _parserd corpus_. Su questi corpus viene allenato il tagger/parser.

>Il **tagger** torna un codice parlante per ogni lessema, identificativo della sua POS rispetto al contesto, quindi fa analisi grammaticale ma non logica. È più semplice del parser dato che non costruisce un albero sintattico.

POS - **Part Of Speech** -> analisi grammaticale -> codice parlante token by token
