# Introduzione
## Nella vita quotidiana
Nella vita quotidiana capita di fare usato di tolerant retrieval nella ricerca su Google: *did you mean  ...?*

## Wildcard
Il secondo caso d'uso è la ricerca con wildcard.

## Riassumendo
La tolerant retrieval viene usata in contesti di:
- **word spelling correction**
- **wildcard**
- **soundex**

# Wildcard
Tipi di wildcard:
- **prefix** queries
- **postfix** queries
- **infix** queries
- prefix + postfix = substring

## Prefix query
Data una query tipo `mon*`, se il vocabolario dell'inverted index è ordinato, non serve null'altro. Anche l'implementazione con BTree o il trie è sufficiente.
Il requisito minimo è il supporto per le range queries, ovvero basta poter accedere in ordine lessicografico alla DS.
Nell'esempio devono essere recuperati tutti i termini $t$ nell'intervallo $mon \leq t \lt moo$
#Nota che $moo$ è escluso

## Postfix query
Non mi basta il solo dizionario dell'InvertedIndex.

Si può pensare di memorizzare le parole in un dizionario/**lista parallela** dove l'ordinamento avviene da destra a sinistra. Il problema del postfix si riconduce a quello del prefix.

Ad esempio: `*mon` viene trasformata in `nom*` sul dizionario delle parole invertite

## Infix query
Esempio: `m*nchen`

Intuizione 1: posso spezzarla in 2 query, una prefissa e una postfissa, e intersecare i due risultati (termini del vocabolario). Soluzione molto costosa.

### Permuterm index
Intuizione 2: **permutation index** - per ogni vocabolo si generano tutte le possibili rotazioni di ogni parola. Ad ogni parola è associata la parola originaria. Anche la query viene ruotata fino a farla diventare una prefix query. Il numero di rotazioni è uguale al numero di lettere nella parola più uno (in quanto viene aggiunto un simbolo terminatore), fa esplodere la dimensione dell'indice, ma le performance temporali sono molto buone.

Nomi alternativi per il permutation index: **permuterm** o **permuindex**. Spesso accade che le rotazioni di ogni parola siano salvate in un trie; ogni foglia punta a una entry dell'inverted index.

Permutazioni di `hello$`:
- `hello$`
- `ello$h`
- `llo$he`
- `lo$hel`
- `o$hell`
- `$hello`

Il vantaggio di questa DS è il supporto per TUTTI i tipi di query visti fin'ora. 
Anche la ricerca di sottostringhe: `*X*` diventa `X*`
#Nota che non è presente il terminatore

Costo: per l'inglese, quadruplica la dimensione del lexicon, mostrano osservazioni empiriche.

### Q-Gram
>È una sequenza di $q$ caratteri della stringa originale. Si crea una **sliding/rolling window** dalla dimensione $q$

Per non perdersi i caratteri iniziali e finali si aggiungono in testa e in coda un prefisso e un suffisso di lunghezza $q-1$. `vacations` con `q=3`: `##vacations$$`
- `##v`
- `#va`
- `vac`
- `aca`
- ...
- `s$$`

Si usano anche per word embedding e altro.

Nomenclatura: bigrammi, trigrammi, quadrigrammi, etc.

Il numero di q-grammi è $L+q-1$ considerando come $L$ la lunghezza originale della parola, senza prefissi e suffissi artificiali.

Si costruisce una **q-gram index**: una struttura, analoga all'inverted index, in cui il vocabolario è costituito dall'insieme dei q-grammi, mentre le posting list sono le parole che contengono il q-gramma. Questa è una struttura *accessoria*.

Il q-gram index è una struttura dati molto compatta, dato che ha lunghezza di ogni entry nota e dimensione calcolabile. Inoltre, ogni q-gram è comune a più parole.

Al momento della query: si divide ogni parola della query in q-grammi e si cerca l'intersezione tra le posting list del q-gram index che fanno match.

Usando la wildcard star nel modo seguente: `mon*` si ottiene la lista di bigrammi `{#mo, mo, on}`, quindi si cerca l'intersezione tra le posting list del q-gram index. Questa intersezione ritorna tutte le parole che fanno match con la query che contiene la wildcard + alcuni falsi positivi come `moon`. Questo rende necessario un post-filtering.

### Confronto
Permuterm vs. Q-Gram index
- Q-Gram index è più space-efficient
- Permtuerm index non richiede post-filtering

# Spell correction
Usata per:
- query correction
- document correction (solo se strettamente necessario)

In diverse modalità:
- **Isolated word**. Usa informazioni sintattiche per correggere la parola.
- **Context-sensitive** (più costosa e tipicamente evitata). Usa informazioni semantiche della parola, legate al contesto, per la correzione.

## Document correction
Usata a valle di OCR per sistemare gli errori di riconoscimento del sistema.

## Isolated word correction
Premessa 1: **lexicon** (lista di lessemi ed elementi lessicali di una lingua) da cui attingere per la correttezza delle parole. Scelte:
- standalone lexicon come Webster's dictionary o industry-specific lexicon
- lexicon del corpus indicizzato

Premessa 2: metrica per calcolare la distanza tra la parola sbagliata e le parole corrette. È una misura **similarità sintattica**. Alternative:
- **edit distance**
- **q-gram overlap**

Idea: dato un lexicon e una parola "misspelled", trovare la parola che appartiene al lexicon, con similarità più alta a quella sbagliata.
### Edit distance
>Numero di operazioni minime per trasformare una parola in un'altra. Molto dispendiosa da calcolare.

#Nota la **distanza di Hamming** misura la distanza tra stringhe della stessa lunghezza; infatti sono consentite solo sostituzioni. Appartiene alla famiglia delle edit distance.

Esistono diverse edit distance:
- **Levenshtein** in cui sono consentite aggiunta, rimozione e modifica
- **Damerau-Levenshtein** in cui alle precedenti si aggiunge la trasposizione (scambio di due caratteri)
La seconda distanza penalizza meno l'errore di inversione di caratteri, molto comune nella digitazione su tastiera.

È un problema di DP, che si risolve con la *tabulazione*. Si crea una matrice di $len(s_1)+1$ righe e $len(s_2)+1$ colonne, dove il $+1$ serve ad allocare lo spazio per la stringa vuota.
Si inizializza il tabulato con una sequenza progressiva da 0 in poi: rappresenta il costo di aggiunta della lettera rispetto alla stringa nulla. Ogni cella in cui la lettera sulla riga e sulla colonna sono uguali (diagonale principale) sarà posta a 0.
Il valore della cella corrente sarà uguale al valore minimo dei vicini (SU, SX, DIAG).
La diagonale rappresenta i replacement, righe e colonne rappresentano aggiunte o rimozioni (in base alla direzione nella quale si guardano).
```
c(i, j) = c(i-1, j-1) if x[i] == y[i] else min(c(i-1, j-1), c(i-1, j), c(i-1, j))+1
```

Il costo dell'ED è lineare rispetto alla dimensione del vocabolario e quadratico rispetto al numero dei caratteri. Si possono applicare delle ottimizzazioni, soprattuto se il vocabolario è memorizzato in un trie, dato che ogni nodo condivide un prefisso comune.

Al momento della query: si generano tutte le sequenze di caratteri che hanno una edit distance alla parola malformata, entro un certo limite preimpostato; si interseca questo insieme con il lexicon e si mostrano all'utente correzioni.
Alternative: correzione con la parola più simile appartenente al lexicon; retrieval di tutti i documenti che contengono una delle variazioni.

Ottimizzazione migliore: un filtro basato sui q-grammi. Definita una soglia/legge, si scartano tutti i lessemi che hanno una distanza sicuramente maggiore di una soglia.

### Q-gram overlap: Jaccard similarity coefficient
Dati i q-grammi della parola da correggere e di tutte le parole del vocabolario, possiamo calcolare la similarità dei due insiemi di q-grammi:
$$S = \frac{|X \cap Y|}{|X \cup Y|}$$
È un valore normalizzato: sta tra 0 e 1

L'intersezione insiemistica può essere calcolata efficientemente se gli insiemi sono ordinati. Si usano due puntatori, ...

Al momento della query, le parole plausibili sono tutte quelle tali per cui, fissata una soglia di similarità, la similarità tra la parola e il termine malformato è più alta della soglia.

#### Sul Q-gram index
L'approccio precedente presuppone che sia disponibile l'associazione $parola \rightarrow q-grams$, ma spesso è il contrario; come nel caso del Q-gram index si mantiene l'associazione inversa: q-gram to term.

Per ogni q-gramma della parola da correggere istanzio un puntatore. Muovendo il puntatori alle parole della posting list in parallelo, posso filtrare le parole che appartengono a N Q-grammi sui totali.

Lavorando sui q-grammi ho il vantaggio di lavorare su più parole in parallelo -> O(len(postingList))

#Ricorda che le parole della posting-list sono sempre memorizzate in ordine.

Se fisso una misura di similarità, ad esempio, pari a 2 q-grammi: tutte le parole in cui si allineano i puntatori paralleli (almeno 2 puntatori puntano alla stessa parola), passano il filtro.

## Consigli operativi
- Lucene: `SpellChecker`
- Python: `PyEnchant` nasce per fare spell-checking

## Advanced Q-gram filtering
>Idea: è molto più semplice stabilire che una stringa s1 non matcha una stringa s2 entro una soglia di errore, piuttosto che stabilire in contrario.

Ad esempio: filtrare tutti i lessemi che richiedono una edit distance superiore a 1 equiavale a dividere a metà una parola, e scartere tutti i lessemi che non contengono nessuna metà, in quanto servirebbero almeno 2 operazioni - una per metà - per portare s1 coincidente a s2.

- **Length filtering**: $ed(s1, s2) \leq k \Leftrightarrow abs(len(s1), len(s2)) \leq k$
- **Count filtering**: $ed(s1, s2) \leq k \Leftrightarrow [max(len(s1), len(s2)) + q - 1] - kq$ in comune; infatti ogni operazione consentita dalla distanza di edit modifica $q$ q-grammi. Il massimo sulla lunghezza è dovuto al fatto che l'inserimento comporta l'aggiunta di un carattere, così come la rimozione porta alla perdita di un carattere; il numero di q-grammi affetti da modifica è q solo nella stringa più lunga, q-1 nell'altra.
- **Position filering**: risolvono il "problema" secondo il quale più parole, differenti tra loro, danno vita allo stesso set di q-grammi memorizzando anche la posizione di ognuno di essi.

## Context-based spelling correction
Idea: trovare tutte le variazione sintattiche di una parola entro una certa edit distance. Enumerare il prodotto cartesiano delle variazioni e lanciare le query. Proporre all'utente quella con il maggior numero di hit.

# Soundex
>Algoritmo di correzione fonetica, usato nelle lingue in cui la pronuncia è diversa dalla grafia della parola. Di fatto, è un algoritmo fonetico che indicizza nomi come suoni.

Nasce con il censimento del 1918 a causa dell'alto numero di analfabeti.

## Intuizione
>Mettere insieme parole **omofone** (parole che sono approssimativamente foneticamente simili) identificandole con un codice a 4 cifre.

Al momento della query: in ingresso la parola, in uscita il codice.

Esistono 6 categorie fonetiche:
1. bilabiale
2. labiodentale
3. dentale
4. alveolare
5. velare
6. glottale

Ogni parola vine trasformata in una sequenza di 4 simboli.

## Esempio
Parola in ingresso: **herman** (army man, soldato)

Algo:
1. mantieni la prima lettera della parola
2. cambia tutte le occorrenze di _A E I O U H W Y_ con 0
3. B F P V -> 1
4. etc. (Vedi trasformazioni)
Codifica: **H06505**
5. sostituisci tutte le coppie di numeri consecutivi con una occorrenza
6. rimuovi gli zeri
7. padding con zero prefissi e suffissi


Quindi _Herman_ diventa H655
# Puntatori in avanti
- CS276 @ Stanford
- Peter Norvig

