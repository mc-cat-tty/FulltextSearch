# Introduzione
## Nella vita quotidiana
Nella vita quotidiana capita di fare usto di tolerant retrieval nella ricerca su Google: *did you mean  ...?*

## Wildcard
Il secondo caso d'uso è la ricerca con wildcard.

## Riassumendo
La tolerant retrieval viene usata in contesto di:
- word spelling correction
- wildcard
- soundex

# Wildcard
Tipi di wildcard:
- prefix queries
- postfix queries
- infix queries

## Prefix query
Data una query tipo `mon*`, se il vocabolario dell'inverted index è ordinato, non serve null'altro. Anche l'implementazione con BTree è sufficiente.
Il requisito minimo è il supporto per le range queries, ovvero basta poter accedere in ordine lessicografico alla DS.
Nell'esempio devono essere recuperati tutti i termini $t$ nell'intervallo $mon \leq t \lt moo$
#Nota che $moo$ è escluso

## Postfix query
Non mi basta il solo dizionario dell'InvertedIndex.

Si può pensare di memorizzare le parole in un dizionario/lista parallela dove l'ordinamento avviene da destra a sinistra. Il problema del postfix si riconduce a quello del prefix.

Ad esempio: `*mon` viene trasformata in `nom*` sul dizionario delle parole invertite

## Infix query
Esempio: `m*nchen`

Intuizione 1: posso spezzarla in 2 query, una prefissa e una postfissa, e intersecare i due risultati. Soluzione molto costosa.

### Permuterm
Intuizione 2: **permutation index** - per ogni vocabolo si generano tutte le possibili rotazioni di ogni parola. Ad ogni parola è associata la parola originaria. Anche la query viene ruotata fino a farla diventare una prefix query. Il numero di rotazioni è uguale al numero di lettere nella parola più uno, fa esplodere la dimensione dell'indice, ma le performance temporali sono molto buone.

Nomi alternativi per il permutation index: **permuterm** o **permuindex**

Permutazioni di `hello$`:
- `hello$`
- `ello$h`
- `llo$he`
- `lo$hel`
- `o$hell`
- `$hello`

Il vantaggio di questa DS è il supporto per TUTTI i tipi di query visti fin'ora.

Costo: per l'inglese, quadruplica la dimensione del vocabolario.

### Q-Gram
>È una sequenza di $q$ caratteri della stringa originale. Si crea una **sliding/rolling window** dalla dimensione $q$

Per non perdersi i caratteri iniziali e finali si aggiungono in testa e in coda un prefisso e un suffisso di lunghezza $q-1$. `vacations` con `q=3`: `##vacations$$`
- `##v`
- `#va`
- `vac`
- `aca`
- ...
- `s$$`

Si usano anche per word embedding e altro.

Nomenclatura: bigrammi, trigrammi, quadrigrammi, etc.

Il numero di q-grammi è $L-q+1$

Si costruisce una **q-gram index**: una struttura in cui il vocabolario è costituito dall'insieme dei q-grammi, mentre le posting list sono le parole che contengono il q-gramma. Questa struttura accessoria punta alle entry dell'inverted index.

Il q-gram index è una struttura dati molto compatta, dato che ha lunghezza di ogni entry nota e dimensione calcolabile. Inoltre, ogni q-gram è comune a più parole.

Al momento della query: si divide la query in q-grammi e si cerca l'intersezione tra le posting list del q-gram index che fanno match. Questa intersezione ritorna tutte le parole che fanno match con la query che contiene la wildcard + alcuni falsi positivi. Questo rende necessario un post-filtering.

### Confronto
Permuterm vs. Q-Gram index
- Q-Gram index è più space-efficient
- Permtuerm index non richiede post-filtering

# Spell correction
Usata per:
- query correction
- document correction (solo se strettamente necessario)

In diverse modalità:
- Isolated word
- Context-sensitive (più costosa e tipicamente evitata)

## Document correction
Usata a valle di OCR per sistemare gli errori di riconoscimento del sistema.

## Isolated word correction
Premessa 1: **lexicon** da cui attingere per la correttezza delle parole. Scelte:
- standard lexicon come Webster's dictionary
- industry-specific lexicon

Premessa 2: metrica per calcolare la distanza tra la parola sbagliata e le parole corrette. È una **similarità sintattica**. Alternative:
- **edit distance**
- **q-gram overlap**

### Edit distance
>Numero di operazioni minime per trasformare una parola in un'altra. Molto dispendiosa da calcolare.

#Nota la differenza rispetto alla distanza di Hamming è che quest'ultima misura la distanza tra stringhe della stessa lunghezza

Esistono diverse edit distance:
- Levenshtein in cui sono consentite aggiunta, rimozione e modifica
- Damerau-Levenshtein in cui alle precedenti si aggiunge la trasposizione (scambio di due caratteri)
La seconda distanza penalizza meno l'errore di inversione di caratteri, molto comune nella digitazione su tastiera.

È un problema di DP, che si risolve con la tabulazione. Si crea una matrice di $len(s_1)+1$ righe e $len(s_2)+1$ colonne, dove il $+1$ serve ad allocare lo spazio per la stringa vuota.
Si inizializza il tabulato con una sequenza progressiva da 0 in poi: rappresenta il costo di aggiunta della lettera rispetto alla stringa nulla. Ogni cella in cui la lettera sulla riga e sulla colonna sono uguali (diagonale principale) sarà posta a 0.
Il valore della cella corrente sarà uguale al valore minimo dei vicini (SU, SX, DIAG).
La diagonale rappresenta i replacement, righe e colonne rappresentano aggiunte o rimozioni (in base alla direzione nella quale si guardano),

Il costo dell'ED è lineare rispetto alla dimensione del vocabolario e quadratico rispetto al numero dei caratteri. Si possono applicare delle ottimizzazioni, soprattuto se il vocabolario è memorizzato in un trie, dato che ogni nodo condivide un prefisso comune.

Ottimizzazione migliore: un filtro basato sui q-grammi. Definita una soglia/legge, si scartano tutti i lessemi che hanno una distanza sicuramente maggiore di una soglia.

### Jaccard coefficient
Dati i q-grammi della parola da correggere e di tutte le parole del vocabolario, possiamo calcolare la similarità dei due insiemi di q-grammi:
$$S = \frac{|X \cap Y|}{|X \cup Y|}$$
È un valore normalizzato.

L'intersezione insiemistica può essere calcolata efficientemente se gli insiemi sono ordinati. Si usano due puntatori, ...

Lavorando sui q-grammi ho il vantaggio di lavorare su più parole in parallelo -> O(len(postingList))

#Ricorda che le parole della posting-list sono sempre memorizzate in ordine.
Se fisso una misura di J., ad esempio, pari a 2: tutte le parole in cui si allineano i puntatori paralleli (almeno 2 puntatori sul numero di q-grammi puntano alla stessa parola).

## Consigli operativi
- Lucene: `SpellChecker`
- Python: `PyEnchant` nasce per fare spell-checking

# Soundex
>Algoritmo di correzione fonetica, usato nelle lingue in cui la pronuncia è diversa dalla grafia della parola.

Nasce con il censimento del 1918 a causa dell'alto numero di analfabeti.

## Intuizione
>Mettere insieme parole **omofone** (parole che sono approssimativamente foneticamente simili) identificando ogni gruppo con un codice.

Al momento della query: in ingresso la parola, in uscita il codice.

Esistono 6 categorie fonetiche:
1. bilabiale
2. labiodentale
3. dentale
4. alveolare
5. velare
6. glottale

Ogni parola vine trasformata in una sequenza di 4 simboli.

## Esempio
Parola in ingresso: **herman** (army man, soldato)

Algo:
1. mantieni la prima lettera della parola
2. cambia tutte le occorrenze di _A E I O U H W Y_ con 0
3. B F P V -> 1
4. etc. (Vedi trasformazioni)
Codifica: **H06505**
5. sostituisci tutte le coppie di numeri consecutivi con una occorrenza
6. rimuovi gli zeri
7. padding con zero prefissi e suffissi

# Puntatori in avanti
- CS276 @ Stanford
- Peter Norvig

