# Introduzione
Per stimare la bontà di un sistema vogliamo trovare metriche che misurino le performance in termini di tempo e spazio.

Tempo platform-independent: MIPS e I/O per second

## Metriche per sistemi di IR
Efficienza:
- Velocità di indicizzazione: numero di documenti all'ora, sulla dimensione media del documento.
- Velocità di ricerca: latenza come funzione della dimensione dell'index. Plot del tempo di ricerca in funzione dell'index size. Da un'idea della scalabilità del sistema.
- Espressività del linguaggio di interrogazione: abilità di esprimere necessità informative complesse; velocità di query complesse.
- Uncluttered UI: quanto l'interfaccia utente è in ordine, utilizzabile e fruibile

Efficacia:
- felicità nei risultati restituiti (*user happiness* - soddisfazione dell'utente)
- rilevanza: risultati attendibili/rilevanti
- non rilevanza: risultati che non vorrei

## Measuring user happiness
Misurare la felicità di un utente dipende in prima istanza dall'utente che sto cercando di accontentare.

#Vedi Dan Russel

*La felicità è una misura elusiva*. Il proxy che useremo è la **rilevanza** dei risultati.

Come misurare la rilevanza?
## Rilevanza
Misurare la rilevanza richiede 3 elementi:
1. collezione dei documenti su cui viene effettuato il benchmark
2. insieme delle query con cui si fa benchmarking
3. una misura binaria della rilevanza per ogni query su ogni documento (è o non è rilevante)

#Nota con rilevanza si intende che il risultato (documento) è rilevante per la UIN espressa dalla query.
#Attenzione la rilevanza non è relativa alla query, non sarebbe misurabile.

### Recall e Precision
Data la query $q$

Sia $R$ l'insieme dei documenti rilevanti (rispetto alla UIN)
Sia $A$ l'insieme della risposta del sistema di IR (Answer)
$R_a$ intersezione tra $R$ e $A$

Abbiamo due misure, ottenute dal rapporto tra $R_a$ e, rispettivamente, $R \setminus R_a$ e $A \setminus R_a$

#Nota che R non è conosciuto integralmente a priori

>$Recall = \frac{|R_a|}{|R|}$ veri positivi sui rilevanti
>$Precision = \frac{|R_a|}{|A|}$ veri positivi sul totale ritornato

All'aumentare della recall la precision cala. Se ritorno tutti i documenti del corpus la recall va a 1.
La recall è una funzione non decrescente rispetto al numero di documenti ritornati. Aka aumentando il numero di documenti ritornati aumenta la recall, ma cala la precision.

Al contrario, una precision a 1, porta ad una recall molto bassa: tutti i documenti ritornati sono significativi, ma ne perdo molti altri.

Precision vs Recall è inversamente proporzionale. Desideriamo 1, 1 ma non è possibile.

Questa metrica non tiene in conto il raking dei documenti. Non devo più considerare il risultato come un set non ordinato. In questo modo valutiamo anche la bontà del modello di ranking.

L'idea è quella di misurare la precision per ogni lista ordinata che contiene fino ad un documento rilevante. La lista si estende spostando la fine di elemento rilevante in elemento rilevante, inglobando anche documenti non rilevanti (tra la testa e la fine della lista).

La precision scende a zero quando si inglobano più elementi del numero di elementi rilevanti che sono stati trovati dalla query.

Esempio (x per risultati rilevanti):
```
|Ra| = 4 |R| = 6 |A| = 8

IR1                      IR2
|x| P(1/6) = 1           | |
---
|x| P(2/6) = 1           | |
---
| |                      |x|
|x| P(3/6) = 3/4         |x|
---
| |                      | |
|x| P(4/6) = 4/6         | |
---
| | P(5/6) = 0           |x|
| | P(6/6) = 0           |x|
```

Preferisco IR1 a IR2
#Nota la curva sta sopra a quella di IR2, almeno per i primi valori

### Standard recall levels
**Livelli di recall naturali** si muovono sul numero di documenti rilevanti. Nel caso sopra: P(R = 1/6), P(R = 2/6), ..., P(R = 6/6)

I **livelli di recall standard** si muovono rispetto a 11 valori di recall - da 0% a 100% - e permettono di mettere a confronto sistemi diversi (dato che ho dei punti e non una funzione continua): P(R = 0), P(R = 0.1), ..., P(R = 1)

Tipicamente è frutto di una interpolazione, detta *ceiling interpolation*. Per ogni intervallo della standard recall prendo il valore massimo dei precision appartenente a quel livello.
$$P(R_j) = max_{R_j \leq R \leq R_{j+1}}P(R)$$

Questo torna utile anche per calcolare la precisione media per query differenti, per valutare la precision **complessiva del sistema**. Per calcolare la precisione media per ogni livello di recall standard, per la i-esima query, $P_i(R)$ è la precisione per query i-esima al livello di recall $R$. La precisione media su Nq query al livello di recall R:
$$\bar P(R) = \sum_{i=1}^{N_q} \frac{P_i(R)}{N_q}$$


Se invece siamo interessati a confrontare due sistemi rispetto ad una **specifica query**, vogliamo un *single precision value* che riassuma la precision rispetto a differenti recall:
  - **Interpolated average precision**: $\bar P_q = \sum_{r=0}^n \frac{P_q(r)}{n}$ con r che assume n+1 standard recall levels
  - **Non interpolated average precision**: $\bar P_q = \sum_{r=0}^{|R_q|} \frac{P_q(r/|R_q|)}{|R_q|}$

Un valore cumulativo rispetto a query e recall è la **MAP - Mean Average Precision**, una media delle *average precisions* per più queries.

#Nota con R-precision ci si riferisce alla precision all'R-esimo risultato del ranking

### Problemi
- assunzione batch processing (vs interattività sistemi moderni)
- conoscenza dettagliata dei documenti
- linear orderin necessario, non funzionerebbe con weak ordering
- R e P sono due misure correlate che catturano aspetti differenti: sarebbe meglio una misura unica (vedi sotto)

### Media armonica - F measure
$$F = \frac{2}{\frac{1}{P}+\frac{1}{R}}$$
### E measure
#Vedi formula -> parametrizzata su b, che permette di dare maggiore importanza alla recall o alla precisio
### Discounted Cumulative Gain
Idea: il documento può avere un'intensità di rilevanza; in particolare, i primi documenti mostrati sono più utili di quelli mostrati in fondo.

Assunzioni:
- Più è bassa la posizione del documento all'interno del risultato meno è rilevante per l'utente
- Più è alta la posizione più è utile

La funzione di decadimento (decay) modella il decadere di rilevanza per l'utente del documento.

Il guadagno cumulativo (semplice) è:
$$CG = r_1 + r_2 + ... + r_n$$

Dove $r$ è il valore di rilevanza assegnato al documento. Per esempio, può essere un valore dell'insieme {0, 1, 2, 3}

>Discounted: la rilevanza del doc viene scontata in funzione della posizione

$$DCG = r_1 + \frac{r_2}{log_22} + \frac{r_3}{log_23} + ... + \frac{r_n}{log_2n}$$
Fissata una base per il logaritmo

#Nota non richiede la conoscenza completa di tutti i documenti rilevanti, ma ogni documento rilevato deve avere un grado di rilevanza.

La funzione $DCG$ è non decrescente, non ha un limite superiore. Per normalizzare il valore bisogna conoscere il *ground thruth*, ovvero il valore di DCG per un _ideal ranking_ - ritorna i documenti in ordine decrescete di rilevanza - e fornisce $MaxDCG$


#Vedi SeaBorn library
## Come collezionare ratings
Problema:
- sono costosi (time and financially consuming)
- Inconsistenti tra valutatori e nel tempo
- non sempre rappresentativi degli utenti reali

Cosa fare? si possono studiare le interazioni degli utenti - **user click**, oppure valutare a mano il risultato del ranking.

# Dettagli operativi
Un sistema di IR si può valutare operativamente mediante un set di documenti standard, un set di query standard e un set di documenti rilevanti associato a queste.

Si da in pasto al sistema la repo di documenti standard, successivamente si passano le query e per ogni query si conosce il sottoinsieme rilevante.

Es di benchmark collections:
- SMART collection
- TREC - Text Retrieval Conference organizzata dal NIST - National Institute of Standards and Technology

Basta implementare la logica per la metrica di benchmarking

#Vedi Beir

## Snippets
Snippet statici (query independent) e dinamici (query dependend) rendono più affidabili strategie di valutazione della bontà del ranking sulla base degli user-click, in quanto evitano il "surfind" tra le pagine alla ricerca di quella desiderata