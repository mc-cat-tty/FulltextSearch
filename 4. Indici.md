# Ripasso
## Tabelle hash
È una delle DS più comuni per l'implementazione di dizionari: implementano la relazione $r: D -> C$ dove D è l'insieme delle chiavi e C l'insieme dei valori.
Nelle tabelle di hash le tuple $(k, v)$ vengono memorizzate in un array, chiamato $H$ di dimensione $m$. La chiave può essere qualsiasi cosa, sotto l'assunzione che sia convertibile in un valore intero.

Una funzione di hash $h$ con insieme universo $u$ si dice **perfetta** se: $\forall k_1 : k_2 \in u, k_1 \neq k_2 \rightarrow h(k_1) \neq h(k_2)$

Per $|u|$ grandi ha lo svantaggio di lasciare (potenzialmente) molte posizioni di $H$ libere

In generale, le funzioni di hash non sono perfette: il numero di chiavi è maggiore delle posizioni di $H$ -> **collisioni**

Si cerca un modo per ammettere che più chiavi vengano mappate nella stessa cella di H:
- liste di trabocco
- indirizzamento aperto

### Liste di trabocco
Ogni posizione di H mantiene a sua volta una lista. Nel caso peggiore, con $|H| = 1$, la tabella di hash si comporta come una lista/linked-list.

Il fattore di carico (nonchè la lunghezza attesa delle liste, se h uniforme) è: $\alpha = \frac{n}{m}$; dove n è il numero di chiavi utilizzate, mentre m è la dimensione della tabella di hash.
Con $\alpha \gt 1$ è sicuramente presente almeno una collisione.

### Indirizzamento aperto
Estendendo la funzione di hash con un numero di ispezione in modo tale che $\forall i \neq j \rightarrow h(k, i) \neq h(k, i)$

La sequenza delle funzioni di hash al progredire del numero di ispezione forma una permutazione degli indici di H. Sarebbe ottimale che ogni chiave avesse la stessa probabilità di avere come sequenza di ispezione una delle $m!$ permutazioni di $[0, ..., m-1]$

Funzione di **ispezione lineare**: $h(k, i) = (hash(k, i) + c \cdot i) \mod m$
Con valori di $c$ bassi tendono a crearsi agglomerat, che rallentano le operazioni (ricerca, inserimento).

Può avvenire che la tabella di hash si saturi: la strategia solitamente utilizzata è quella della riallocazione. Quando si supera una soglia del fattore di carico pari a 0.5/0.75 si raddoppia la dimensione della lista.

## B-tree
>È un albero auto-bilanciante che consente ogni operazione in tempo logaritmico.

#Vedi CS61b UCB
### Analisi del problema
Parametri di un albero:
- depth - distanza di un nodo dalla radice. La distanza della radice da se stessa è 0
- height - max(depth) - determina il worst case delle operazioni (uguale a height+1)
- average depth - avg(depth) - determina il caso medio (uguale a avg+1)

Perchè i BST non sono sufficienti? considerando un ordine di inserimento ed eliminazione di valori in un BST casuale, esso sarebbe sommariamente bilanciato, con una profondità media intorno a 
$2logN$ e una altezza di circa $4lognN$, dimostrato empiricamente.

Applicazioni reali inseriscono i valori in successione, ad esempio le date di un sistema di logging, rendendo l'altezza ben più alta.

### Idea
>Mai aggiugnere foglie in fondo all'albero, piuttosto sovraffollare il contenuto di una foglia con più valori, fino a raggiungere un limite (eg. L=3) di capacità massima.

### Implementazione
Per le foglie: raggiunto il limite L di elementi per foglia, si sceglie uno split item (con L<3 il centrale sinistro) che sale verso il nodo sopra di esso, mentre si splitta la sequenza in più foglie.

#Nota il numero di sottoalberi di un nodo con al massimo L elementi è L+1

Per i nodi intermedi: vale lo stesso; al momento dello split si genereranno due sottoalberi.
Per la radice: anch'essa può subire split. Lo split item diventa la nuova radice, mentre gli elementi a destra e a sinistra diventano figli della radice.

Se usato per rappresentare un dizionario, tipicamente si sceglie di incorporare il valore associato ad ogni chiave, direttamente all'interno del nodo.
### Origine del nome
Cosa significa B-tree? Bayer, Bushy, Board, Balanced?

Sono anche chiamati alberi 2-3-4 o 2-4, dal numero variabile di figli che ogni nodo può avere.

## Red-black tree
Sono alberi che usano operazioni di rotazione per bilanciare i BST.
#Vedi CS61b UCB

## B+tree
Migliorano i B-tree e hanno le seguenti caratteristiche:
- le foglie contengono tutte le chiavi possibili, quindi le chiavi sono ripetute nei nodi intermedi dell'albero
- solamente le foglie hanno data pointer (memorizzano il valore associato alla chiave). Inoltre memorizzano il puntatore alla foglia successiva (le foglie sono memorizzate come linked list) per consentire un accesso sequenziale ai dati.
L'idea è avere un albero n-ario che acceleri l'accesso ad una struttura dati  sequenziale già esistente. Vengono infatti usati per l'accesso al disco mediante fs.
# Introduzione
> Gli indici sono strutture dati che accelerano l'accesso (ricerca) di dati, conservate sulla **memoria secondaria**.

> Gli indici sono strutture dati costruite sopra a un testo (nel nostro caso) che permettono di velocizzare la ricerca. Gli indici si costruiscono offline: store first, query later

Il processo di indicizzazione è un processo separato dal query processing.

Gli indici che vengono usati per fare ricerca su dati alfanumerici (ad esempio un codice fiscale) sono chiamati **B+ tree** (*B plus tree*). È la principale tipologia di indice che viene usato per fare ricerche su grandi quantità di dati, perché il costo di accesso è logaritmico; non logaritmo in base due, ma base più alta - si dice infatti che i B+ tree hanno alto **fanout** -, che permette performance migliori.

In SQL si costruisce sulle colonne su cui faccio accesso più di frequente.

#x012 Vedi alberi 2-3-4, alberi AVL

L'indicizzazione funziona quando il tasso di modifica della collezione dei dati è di qualche ordine inferiore rispetto al tasso di ricerca: è **semistatica** e di **grandi dimensioni**; ovvero quando il gioco vale la candela. Flussi di dati "in streaming" non vengono indicizzati.

Tecniche di indicizzazione:
- **inverted indexes**. Composto da vocabolario e occorrenze. Scelta migliore al momento.
- **suffix arrays**. Utilizzati per fare matching di catene proteiche.
- **signature files**. Concorrenti degli inverted indexes ma in disuso.

Cosa dobbiamo considerare?
- search cost +
- space overhead -
- costo di creazione e aggiornamento -

Solitamente il bilancio è positivo per gli inverted indexes.
## Inverted index
> Gli **inverted index** sono dizionari che contengono come chiave una parola e come valore la lista di documenti che la contengono.

## Notazione
- **n** dimensione del text repository. Si ottiene concatenando tutti i caratteri di tutti i token di tutti i documenti contenuti nel database.
- **m** lunghezza della parola da cercare. "letter" è lunga 6
- **M** dimensione della memoria principale, misurata nella stessa unità di misura dei restanti, ad esempio in caratteri.

Costo ricerche:

| nome | costo ricerca | costo inserimento |
|------|---------------|-------------------|
|sorted array|logN|N|
|binary tree|logN|N|
|hash table|N |N|
|B tree|logN|logN|

# Trie o Prefix Tree
>Albero n-ario in-memory (memoria principale) per memorizzare un insieme di stringhe, presenti nelle foglie.

Albero su cui ogni arco è etichettato con una lettera, sulle foglie parole complete con le loro posizioni. Sui nodi intermedi lettere, parole parziali, ma anche altre parole, più corte di altre che si troveranno sotto di loro. In ogni nodo oltre ad una stringa si trova anche una lista di documenti.

> Fissato un nodo tutti i suoi discendenti hanno in comune un prefisso, che può essere calcolato camminando sull'albero, dalla radice al nodo in questione

#x013 Cosa succede se voglio inserire una nuova parola? nel punto in cui fallisce la ricerca aggiungo un nodo di split con un arco uscente etichettato con la lettera che caratterizza la parola già presente e il secondo arco uscente etichettato con la lettera caratterizzante la parola da aggiungere.

Nel caso peggiore costa *m*, non dipende dalla dimensione del vocabolario.

# Dettagli sull'inverted index
Un indice è il contrario di una ricerca lineare o sequenziale. Quest'ultima viene usata solo se il dataset è volatile, oppure se si sta esplorando il dataset (eg. crawler)

Voglio eseguire la query: opere di Shakespare che contengono Brutus and Calpurnia but not Cleopatra

Uso una **matrice di incidenza** dove sulle righe stanno i personaggi, mentre sulle colonne le opere, per ogni cella un valore binario che rappresenta presenza o assenza del personaggio nell'opera.

#x014 
Le matrici di incidenza non scalano. Sono grandi entry * documenti
Le matrici di incidenza sono tipicamente sparse, tante più sparse quanti sono i documenti su cui indicizzo.

Gli inverted index tengono traccia solo degli 1; come entry uso le parole, che puntano a liste di documenti che le contengono. Struttura dati orientata alla parola.

Non è sicuramente una struttura dati statica -> **posting list** -> è una lista di *document IDs*

## Definizione
>Meccanismo orientato alla parola per indicizzare una collezione di testi al fine di velocizzare la ricerca.

Si compone di due elementi:
- Vocabolario: insieme di tutte le parole contenute nei testi, insieme alla loro document frequency all'interno del corpus (utile più avanti per il ranking)
- **Posting list** o lista delle occorrenze: per ogni parola può memorizzare la posizione dell'occorrenza
	- **document based**: lista di documenti con la corrispondente *tf* (term-frequency) all'interno del documento. Solitamente si memorizzano anche informazioni aggiuntive che indicano l'importanza del termine, ad esempio. In Google se la parola si trova nel corpo o nel testo.
	- **word based**: lista di documenti con la posizione corrispondente (utile per word-retrieval, più fine)

## Heap's law
Legge appartenente alla teoria dell'informazione.

Un vocabolario cresce con $O(n^b) : 0 \lt b \lt 1$. Solitamente $b = 0.4/0.5$

Cresce con un andamento semi-logaritmico fino ad arrivare a un plateau in cui si esaurisce il vocabolario.

La dimensione si può ridurre facendo stemming o lemmatization

Ad esempio, nella collezione TREC-3, lo spazio richiesto dal dizionario per 1Gb di testo è di soli 5Mb.

Lo spazio extra del word-based è lineare nella dimensione del dataset, in particolare si aggira intorno al 40%.
Risparmio dal 20% al 40% con un document-based inverted index.

# Costruzione dell'indice
```Python
def build_index(docs):
	index = dict()
	for doc_idx, doc in enumerate(docs):
			tokens = set(tokenize(doc))
			for token in tokens:
				if not token in index: index[token] = list()
				index[token].append(doc_idx)
	return index
```
# Memorizzazione del vocabolario
#Ricorda che ogni struttura dati non lineare deve essere serializzata per poterna tenere in memoria.

Cosa succede se saturo la memoria principale? L'algoritmo di costruzione fallisce. Una strategia per risolvere il problema consiste nel creare più inverted index di dimensioni minori, fare il commit di ognuno di essi in memoria secondaria; al momento della lettura è sufficiente fare merge delle posting list appartenenti allo stesso termine: se queste liste sono già ordinate (vedi dopo), il merge costa poco.
## Disjoint set
>È una struttura dati che memorizza una collezione di partizioni disgiunte (sottoinsieme disgiunti) di un insieme generativo. Ammette oprerazioni di ricerca e unione. Chiamato anche union-find set o merge-find set.

Ammette le primitive:
- makeset(X) -> ritorna un DS in cui ogni partizione contiene un solo elementi
- union(DS, x, y) -> unisce gli insiemi che contengono x e y
- findset(DS, x) -> ritorna il rappresentante del set che contiene x

Può essere memorizzato mediante una foresta di alberi, in cui ogni albero è un set. Ogni set è individuato da un rappresentante, nonchè la radice dell'albero, caratterizzata da un arco entrante verso sè stessa (cappio).
### Sort-merge join
È un algoritmo di join usato in DB relazionali. L'obiettivo è diminuire i confronti nella fase di merging. Si ordinano le due tabelle sulle colonne di join; ora, con un doppio puntatore, è facile rilevare gli elementi comuni. Nel caso peggiore $O(N+M+NlogN+MlogM)$, dove $XlogX$ è più impattante delle parte lineare.

---

Solitamente si mantiene un vocabolario separato dalle posting list. In particolare voglio che il vocabolario sia caricato in memoria principale, mentre le posting list in memoria secondaria.
Quando voglio accedere alla lista di documenti che contengono una parola entro nel vocabolario, mediante una ricerca dicotomica, trovo la posting list e accedo a questa.

- Sul vocabolario posso costruire il Trie, che ha un costo di accesso pari alla lunghezza della parola. Mediamente le parole sono lunghe 6 lettere. Overhead spaziale.
- Il B+ tree ha un costo di accesso logaritmico con base m maggiore di 2.
- L'hash non è **order-preserving**, quindi non posso fare *range query*

Il vocabolario è strutturato come array ordinato con *offset, dimensione* della posting list corrispondente.

## Google inverted index
- distribuito su circa 3M di server
- l'indice è stato partizionato in più blocchi, chiamati *shards*
- ogni shard su più repliche

#Vedi Caffeine, in nuovo algoritmo di ricerca di Google, che fa continuo crawling e updating

# Ricerca su inverted index
#Vedi: ricerche booleane, proximity queries

1. ricerca sul vocabolario dei token coinvolti nella ricerca
2. recupero delle occorrenze

Tipi di ricerca:
- single words queries
- prefix and range queries (eg: tutte le posting lists delle parole comprese tra *parola iniziale* e *parola finale*). Queste query possono essere risolte con tutte le strutture viste (sorted vocabulary, trie, B+ tree), tranne hash, in quanto non è order-preserving.

## Boolean retrieval
Data una query booleana, parsata secondo la grammatica degli operatori logici e costruisco l'albero sintattico. Ogni foglia è una parola. (Nella radice avrò l'ultima operazione da eseguire)

Ripercorro l'albero sintattico e applico l'operazione inclusa nel nodo:
- AND -> intersezione delle posting list
- OR -> unione delle posting list
- NOT -> set difference dell posting list

Un algoritmo non primitivo di unione, intersezione e differenza costa O(N) e usa due puntatori per scorrere le due posting lists.

## Phrasal retrieval
Premessa: devo avere un inverted index word based.

Idea:
- recupero lista di file e posizioni associate per ogni parola della query
- calcolo l'intersezione tra le posting list sopra estratte
- check della contiguità
### Contiguity check
Se ho la query "computer science engineering":
- applico la query *computer AND science AND engineering*
- per ogni parola recupero la posting list, ottengo il set di documenti D dall'intersezione delle posting list
- prendo la lista con il minor numero di occorrenze e la fisso
- per ogni posizione della lista più corta cerco nelle altre liste la contiguità. #x004 

Dato un set di documenti D in cui compaiono tutte le keyword $(k_1, k_2, k_3, ..., k_m)$ della frase (estratto con l'algoritmo di cui sopra):
- per ogni documento $d \in D$
- seleziona la posting list più corta $P_s \in d$ tra le posting list del termine nel documento
- fissata la posting list, per ogni posizione di occorrenza $p$ della keyword $k_s$
	- per ogni posting list $P_i : P_i \neq P_s$, cerco la presenza di $p-s+i$. Se non è presente viene scartata l'occorrenza
- se, con la ricerca descritta sopra, trovo una posizione di occorrenza che soddisfa tutte le parole, inserisco il documento nel set dei risultati


#Vedi Lucene, da cui derivano Solr e elasticsearch. È l'inverted index di Apache
#Vedi tecniche di ricerca su indici compressi

## Proximity search
Rilassa in concetto di phrasal retrieval consentendo un range di distanza tra le parole della query.