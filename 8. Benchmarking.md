# Introduzione
Per stimare la bontà di un sistema vogliamo trovare metriche che misurino le performance in termini di tempo e spazio.

Tempo platform-independent: MIPS e I/O per second

## Metriche per sistemi di IR
Efficienza:
- Velocità di indicizzazione: numero di documenti all'ora, sulla dimensione media del documento.
- Velocità di ricerca: latenza come funzione della dimensione dell'index. Plot del tempo di ricerca in funzione dell'index size. Da un'idea della scalabilità del sistema.
- Espressività del linguaggio di interrogazione: abilità di esprimere necessità informative complesse; velocità di query complesse.
- Uncluttered UI: quanto l'interfaccia utente è in ordine, utilizzabile e fruibile

Efficacia:
- felicità nei risultati restituiti
- rilevanza: risultati attendibili/rilevanti
- non relevanza: risultati che non vorrei

## Measuring user happiness
Misurare la felicità di un utente dipende in prima istanza dall'utente che sto cercando di accontentare.

#Vedi Dan Russel

*La felicità è una misura elusiva*. Il proxy che useremo è la **rilevanza** dei risultati.

Come misurare la rilevanza?
## Rilevanza
Misurare la rilevanza richiede 3 elementi:
1. collezione dei documenti su cui viene effettuato il benchmark
2. insieme delle query con cui si fa benchmarking
3. una misura binaria della rilevanza per ogni query su ogni documento (è o non è rilevante)

#Nota con rilevanza si intende che il risultato (documento) è rilevante per la UIN espressa dalla query.
#Attenzione la rilevanza non è relativa alla query, non sarebbe misurabile

### Recall e Precision
Data la query $q$

Sia $R$ l'insieme dei documenti rilevanti (rispetto alla UIN)
Sia $A$ l'insieme della risposta del sistema di IR (Answer)
$R_a$ intersezione tra $R$ e $A$

Abbiamo due misure, ottenute dal rapporto tra $R_a$ e, rispettivamente, $R \setminus R_a$ e $A \setminus R_a$

#Nota che R non è conosciuto integralmente a priori

>$Recall = \frac{|R_a|}{|R|}$
>$Precision = \frac{|R_a|}{|A|}$

All'aumentare della recall la precision cala. Se ritorno tutti i documenti del corpus la recall va a 1.
La recall è una funzione non decrescente rispetto al numero di documenti ritornati.

#Completa con figura di precision vs recall. Desideriamo 1, 1 ma non è possibile.

Questa metrica non tiene in conto il raking dei documenti. Non devo più considerare il risultato come un set non ordinato. In questo modo valutiamo anche la bontà del modello di ranking.

L'idea è quella di misurare la precision per ogni lista ordinata che contiene fino ad un documento rilevante. La lista si estende spostando la fine di elemento rilevante in elemento rilevante, inglobando anche documenti non rilevanti (tra la testa e la fine della lista).

La precision scende a zero quando si inglobano più elementi del numero di elementi rilevanti che sono stati trovati dalla query.

Esempio (x per risultati rilevanti):
```
|Ra| = 4 |R| = 6 |A| = 8

IR1                      IR2
|x| P(1/6) = 1           | |
---
|x| P(2/6) = 1           | |
---
| |                      |x|
|x| P(3/6) = 3/4         |x|
---
| |                      | |
|x| P(4/6) = 4/6         | |
---
| | P(5/6) = 0           |x|
| | P(6/6) = 0           |x|
```

Preferisco IR1 a IR2
#Nota la curva sta sopra a quella di IR2, almeno per i primi valori

**Livelli di recall naturali** si muovono sul numero di documenti rilevanti. Nel caso sopra: P(R = 1/6), P(R = 2/6), ..., P(R = 6/6)

I **livelli di recall standard** si muovono rispetto a 11 valori - da 0% a 100% - e permettono di mettere a confronto sistemi diversi (dato che ho dei punti e non una funzione continua): P(R = 0), P(R = 0.1), ..., P(R = 1)

Tipicamente è frutto di una interpolazione, detta *ceiling interpolation*. Per ogni intervallo della standard recall prendo il valore massimo dei precision appartenente a quel livello.
$$P(R_j) = max_{R_j }$$

Questo torna utile anche per calcolare la precisione media per query differenti. Per calcolare la precisione media per ogni livello di recall standard, per la i-esima query $P_i(R) è la precisione per query i-esima al livello di recall $R$. La precisione media al livello di recall

#Completa le due formule

#Completa con le diverse metriche di rilevanza

### Discounted Cumulative Gain
Idea: il documento può avere un'intensità di rilevanza
Assunzioni:
- Più è bassa la posizione del documento all'interno del risultato meno è rilevante per l'utente
- ..

La funzione di decadimento (decay) modella il decadere di rilevanza per l'utente del documento.

Il guadagno cumulativo (semplice) è:
$$CG = r_1 + r_2 + ... + r_n$$

>Discounted: la rilevanza del doc viene scontata in funzione della posizione

$$DCG = r_1 + \frac{r_2}{log_22} + \frac{r_3}{log_23} + ... + \frac{r_n}{log_2n}$$
Fissata una base per il logaritmo

#Nota non richiede la conoscenza completa di tutti i documenti rilevanti, ma ogni documento rilevato deve avere un grado di rilevanza.

La funzione $DCG$ è non decrescente, non ha un limite superiore. Per normalizzare il valore bisogna conoscere il *gold standard*: l'insieme di tutti i documenti rilevanti per la UIN e il loro grado di rilevanza: $MaxDCG$

#Vedi SeaBorn library
## Come collezionare ratings
Problema:
- sono costosi (time and financially consuming)
- Inconsistenti tra valutatori e nel tempo
- non sempre rappresentativi degli utenti reali

Cosa fare? si possono studiare le interazioni degli utenti, oppure valutare a mano il risultato del ranking.

## Dettagli operativi
Un sistema di IR si può valutare operativamente mediante un set di documenti standard, un set di query standard e un set di documenti rilevanti associato a questi.

Si da in pasto al sistema la repo di documenti standard, successivamente si passano le query e per ogni query si conosce il sottoinsieme rilevante.

Es:
- SMART collection
- TREC

Basta implementare la logica per la metrica di benchmarking

#Vedi Beir